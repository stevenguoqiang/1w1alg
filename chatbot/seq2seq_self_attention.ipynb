{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3cb88c62f454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multihead_attention(queries,\n",
    "                        keys,\n",
    "                        num_units=None,\n",
    "                        num_heads=8,\n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\",\n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "\n",
    "    Args\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked.\n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "\n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h)\n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h)\n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h)\n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "\n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "\n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "\n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "\n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "\n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "\n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "\n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"encoder\"):\n",
    "    ## Embedding 词向量的计算\n",
    "    self.enc = embedding(self.x,\n",
    "                          vocab_size=len(de2idx),\n",
    "                          num_units=hp.hidden_units,\n",
    "                          scale=True,\n",
    "                          scope=\"enc_embed\")\n",
    "\n",
    "    ## Positional Encoding 加入位置向量\n",
    "    self.enc += positional_encoding(self.x,\n",
    "                          num_units=hp.hidden_units,\n",
    "                          zero_pad=False,\n",
    "                          scale=False,\n",
    "                          scope=\"enc_pe\")\n",
    "\n",
    "\n",
    "    ## Dropout 对输入向量进行dropout处理，减少过拟合\n",
    "    self.enc = tf.layers.dropout(self.enc,\n",
    "                                rate=hp.dropout_rate,\n",
    "                                training=tf.convert_to_tensor(is_training))\n",
    "\n",
    "    ## Blocks，若干个相同的block\n",
    "    for i in range(hp.num_blocks):\n",
    "        with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "            ### Multihead Attention，多头注意力机制\n",
    "            self.enc = multihead_attention(queries=self.enc,\n",
    "                                            keys=self.enc,\n",
    "                                            num_units=hp.hidden_units,\n",
    "                                            num_heads=hp.num_heads,\n",
    "                                            dropout_rate=hp.dropout_rate,\n",
    "                                            is_training=is_training,\n",
    "                                            causality=False)\n",
    "\n",
    "            ### Feed Forward，接上一个全连接层。\n",
    "            self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "with tf.variable_scope(\"decoder\"):\n",
    "    ## Embedding\n",
    "    self.dec = embedding(self.decoder_inputs,\n",
    "                          vocab_size=len(en2idx),\n",
    "                          num_units=hp.hidden_units,\n",
    "                          scale=True,\n",
    "                          scope=\"dec_embed\")\n",
    "\n",
    "    ## Positional Encoding\n",
    "    self.dec += positional_encoding(self.decoder_inputs,\n",
    "                          vocab_size=hp.maxlen,\n",
    "                          num_units=hp.hidden_units,\n",
    "                          zero_pad=False,\n",
    "                          scale=False,\n",
    "                          scope=\"dec_pe\")\n",
    "\n",
    "    ## Dropout\n",
    "    self.dec = tf.layers.dropout(self.dec,\n",
    "                                rate=hp.dropout_rate,\n",
    "                                training=tf.convert_to_tensor(is_training))\n",
    "\n",
    "    ## Blocks\n",
    "    for i in range(hp.num_blocks):\n",
    "        with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "            ## Multihead Attention ( self-attention) ，自注意力机制\n",
    "            self.dec = multihead_attention(queries=self.dec,\n",
    "                                            keys=self.dec,\n",
    "                                            num_units=hp.hidden_units,\n",
    "                                            num_heads=hp.num_heads,\n",
    "                                            dropout_rate=hp.dropout_rate,\n",
    "                                            is_training=is_training,\n",
    "                                            causality=True,\n",
    "                                            scope=\"self_attention\")\n",
    "\n",
    "            ## Multihead Attention ( vanilla attention)，和编码器的输出结果做注意力机制\n",
    "            self.dec = multihead_attention(queries=self.dec,\n",
    "                                            keys=self.enc,\n",
    "                                            num_units=hp.hidden_units,\n",
    "                                            num_heads=hp.num_heads,\n",
    "                                            dropout_rate=hp.dropout_rate,\n",
    "                                            is_training=is_training,\n",
    "                                            causality=False,\n",
    "                                            scope=\"vanilla_attention\")\n",
    "\n",
    "            ## Feed Forward\n",
    "            self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6.7",
   "language": "python",
   "name": "py3.6.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
